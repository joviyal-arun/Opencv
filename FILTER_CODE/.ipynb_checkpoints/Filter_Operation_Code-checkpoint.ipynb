{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410de510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic images and labels\n",
    "np.random.seed(0)\n",
    "\n",
    "num_samples = 10\n",
    "image_size = 28\n",
    "num_classes = 2  # Binary classification (0 or 1)\n",
    "\n",
    "# Create synthetic image data\n",
    "images = np.random.rand(num_samples, image_size, image_size)\n",
    "\n",
    "# Create synthetic labels (0 or 1)\n",
    "labels = np.random.randint(0, num_classes, size=num_samples)\n",
    "\n",
    "# One-hot encoding for labels (for multi-class classification)\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    encoded_labels = np.zeros((len(labels), num_classes))\n",
    "    for i in range(len(labels)):\n",
    "        encoded_labels[i, labels[i]] = 1\n",
    "    return encoded_labels\n",
    "\n",
    "labels_one_hot = one_hot_encode(labels, num_classes)\n",
    "\n",
    "# Display sample images and labels\n",
    "for i in range(num_samples):\n",
    "    plt.imshow(images[i], cmap='gray')\n",
    "    plt.title(f\"Label: {labels[i]}, One-Hot Label: {labels_one_hot[i]}\")\n",
    "    plt.show()\n",
    "\n",
    "# Define a simple CNN architecture\n",
    "# Here, we use only one convolutional layer, one fully connected layer, and a softmax output layer for binary classification.\n",
    "# You can expand this architecture for more complex tasks.\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize weights and biases\n",
    "conv_weights = np.random.randn(num_filters, filter_size, filter_size)\n",
    "conv_bias = np.zeros((num_filters,))\n",
    "fc_weights = np.random.randn(num_filters * (image_size // pool_size) * (image_size // pool_size), num_classes)\n",
    "fc_bias = np.zeros((num_classes,))\n",
    "\n",
    "# Define the sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(input_data):\n",
    "    # Convolutional layer\n",
    "    conv_output = np.zeros((num_samples, num_filters, image_size - filter_size + 1, image_size - filter_size + 1))\n",
    "    for sample in range(num_samples):\n",
    "        for f in range(num_filters):\n",
    "            for i in range(image_size - filter_size + 1):\n",
    "                for j in range(image_size - filter_size + 1):\n",
    "                    conv_output[sample, f, i, j] = np.sum(input_data[sample, i:i+filter_size, j:j+filter_size] * conv_weights[f]) + conv_bias[f]\n",
    "\n",
    "    # Apply sigmoid activation\n",
    "    conv_output = sigmoid(conv_output)\n",
    "\n",
    "    # Max-pooling layer\n",
    "    pooled_output = np.zeros((num_samples, num_filters, image_size // pool_size, image_size // pool_size))\n",
    "    for sample in range(num_samples):\n",
    "        for f in range(num_filters):\n",
    "            for i in range(0, image_size, pool_size):\n",
    "                for j in range(0, image_size, pool_size):\n",
    "                    pooled_output[sample, f, i // pool_size, j // pool_size] = np.max(conv_output[sample, f, i:i+pool_size, j:j+pool_size])\n",
    "\n",
    "    # Flatten the pooled output\n",
    "    flattened = pooled_output.reshape((num_samples, -1))\n",
    "\n",
    "    # Fully connected layer\n",
    "    fc_output = np.dot(flattened, fc_weights) + fc_bias\n",
    "\n",
    "    # Apply softmax activation\n",
    "    exp_fc_output = np.exp(fc_output - np.max(fc_output, axis=1, keepdims=True))\n",
    "    softmax_output = exp_fc_output / np.sum(exp_fc_output, axis=1, keepdims=True)\n",
    "\n",
    "    return softmax_output\n",
    "\n",
    "# Backpropagation\n",
    "def backpropagation(input_data, softmax_output, labels_one_hot):\n",
    "    # Calculate the loss (cross-entropy)\n",
    "    loss = -np.sum(labels_one_hot * np.log(softmax_output)) / num_samples\n",
    "\n",
    "    # Calculate the gradient of the loss with respect to the softmax output\n",
    "    d_loss = (softmax_output - labels_one_hot) / num_samples\n",
    "\n",
    "    # Backpropagate through the fully connected layer\n",
    "    d_fc_weights = np.dot(input_data.T, d_loss)\n",
    "    d_fc_bias = np.sum(d_loss, axis=0, keepdims=True)\n",
    "\n",
    "    # Calculate the gradient of the loss with respect to the flattened pooled output\n",
    "    d_flattened = np.dot(d_loss, fc_weights.T)\n",
    "\n",
    "    # Reshape the gradient to match the shape of the pooled output\n",
    "    d_pooled_output = d_flattened.reshape(pooled_output.shape)\n",
    "\n",
    "    # Backpropagate through the max-pooling layer\n",
    "    d_conv_output = np.zeros(conv_output.shape)\n",
    "    for sample in range(num_samples):\n",
    "        for f in range(num_filters):\n",
    "            for i in range(0, image_size, pool_size):\n",
    "                for j in range(0, image_size, pool_size):\n",
    "                    window = conv_output[sample, f, i:i+pool_size, j:j+pool_size]\n",
    "                    mask = (window == np.max(window))\n",
    "                    d_conv_output[sample, f, i:i+pool_size, j:j+pool_size] = d_pooled_output[sample, f, i // pool_size, j // pool_size] * mask\n",
    "\n",
    "    # Calculate the gradient of the loss with respect to the conv_weights\n",
    "    d_conv_weights = np.zeros(conv_weights.shape)\n",
    "    d_conv_bias = np.zeros(conv_bias.shape)\n",
    "    for sample in range(num_samples):\n",
    "        for f in range(num_filters):\n",
    "            for i in range(image_size - filter_size + 1):\n",
    "                for j in range(image_size - filter_size + 1):\n",
    "                    d_conv_weights[f] += input_data[sample, i:i+filter_size, j:j+filter_size] * d_conv_output[sample, f, i, j]\n",
    "                    d_conv_bias[f] += d_conv_output[sample, f, i, j]\n",
    "\n",
    "    return loss, d_conv_weights, d_conv_bias, d_fc_weights, d_fc_bias\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    softmax_output = forward_propagation(images)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss, d_conv_weights, d_conv_bias, d_fc_weights, d_fc_bias = backpropagation(images, softmax_output, labels_one_hot)\n",
    "\n",
    "    # Update weights and biases using gradient descent\n",
    "    conv_weights -= learning_rate * d_conv_weights\n",
    "    conv_bias -= learning_rate * d_conv_bias\n",
    "    fc_weights -= learning_rate * d_fc_weights\n",
    "    fc_bias -= learning_rate * d_fc_bias\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "def predict(input_data):\n",
    "    softmax_output = forward_propagation(input_data)\n",
    "    return (softmax_output[:, 1] > 0.5).astype(int)\n",
    "\n",
    "# Test the model with new data\n",
    "test_images = np.random.rand(3, image_size, image_size)\n",
    "predictions = predict(test_images)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
